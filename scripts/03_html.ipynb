{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import geojson\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon as shpPolygon\n",
    "\n",
    "from scipy.optimize import differential_evolution\n",
    "from numba import jit\n",
    "\n",
    "from hm import gr4j, gr4j_bounds\n",
    "\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.io import save as save_html\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = pd.read_csv(\"../data/raw_shapes/ESP_meta.csv\")[[\"STATION\", \"NAME\", \"SQ_KM\"]]\n",
    "#consistency\n",
    "meta_data.at[314, \"STATION\"] = 38001\n",
    "meta_data.at[315, \"STATION\"] = 39001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boundaries(polygon):\n",
    "    \"\"\"\n",
    "    Function for receiving boundaries\n",
    "    from the provided .geojson file.\n",
    "    Use for further creating grid polygons\n",
    "    for intersection with our input .geojson polygon\n",
    "    \n",
    "    Input:\n",
    "    .geojson polygon\n",
    "    You can import it using Qgis or \n",
    "    use geojson.io webservice.\n",
    "    \n",
    "    Output:\n",
    "    [west, south, east, north] rounded to integer coordinates\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    lons = [coords[0] for coords in polygon[\"geometry\"][\"coordinates\"][0]]\n",
    "    lats = [coords[1] for coords in polygon[\"geometry\"][\"coordinates\"][0]]\n",
    "    \n",
    "    lon_min, lon_max = np.min(lons), np.max(lons)\n",
    "    lat_min, lat_max = np.min(lats), np.max(lats)\n",
    "    \n",
    "    west = np.min([np.floor(lon_min), np.ceil(lon_min)])\n",
    "    east = np.max([np.floor(lon_max), np.ceil(lon_max)])\n",
    "    south = np.min([np.floor(lat_min), np.ceil(lat_min)])\n",
    "    north = np.max([np.floor(lat_max), np.ceil(lat_max)])\n",
    "    \n",
    "    return [west, south, east, north]\n",
    "\n",
    "def get_era5_polygons_geodf(boundaries, gridsize=0.25):\n",
    "    \n",
    "    \"\"\"\n",
    "    Fuction for obtaining ERA5 grid polygons\n",
    "    for further intersection with .geojson\n",
    "    watershed representation polygon\n",
    "    \n",
    "    Input:\n",
    "    1. boundaries from get_boundaries() function\n",
    "    2. gridsize (default=0.25) of ERA5 dataset\n",
    "    \n",
    "    Output:\n",
    "    Geodataframe (geopandas package data container)\n",
    "    with grid polygons cover watershed area\n",
    "    \n",
    "    \"\"\"    \n",
    "    # read in already prepared lats/lons\n",
    "    lons_c = np.load(\"../data/era5_uk_lons.npy\")\n",
    "    lats_c = np.load(\"../data/era5_uk_lats.npy\")\n",
    "    \n",
    "    # because of the care of geographical latitude consistency\n",
    "    # we need to keep longitude boundaries between -180 and 180\n",
    "    # so, it is not so simple substractinf of 180\n",
    "    # because of they have the same 0 - grinvich meridian\n",
    "    # so we need to transform only values from 180 to 360\n",
    "    # to from 180 to -180\n",
    "    # previous calculations were wrong\n",
    "    # lons_c = lons_c - 180\n",
    "    lons_c = np.where(lons_c > 180, lons_c - 360, lons_c)\n",
    "    \n",
    "    hg = gridsize / 2 # half gridsize\n",
    "    \n",
    "    # save only useful longitudes based on conditioning under boundaries\n",
    "    # +-5 added for instances where is no grid centers \n",
    "    # between boundaries (small catchments)\n",
    "    lons_c = lons_c[(lons_c >=boundaries[0]-5)&(lons_c<=boundaries[2]+5)]\n",
    "    lats_c = lats_c[(lats_c>=boundaries[1]-5)&(lats_c<=boundaries[3]+5)]\n",
    "      \n",
    "    polygons = [shpPolygon([ (x-hg, y-hg), (x-hg, y+hg), (x+hg, y+hg), (x+hg, y-hg), (x-hg, y-hg) ]) \n",
    "                for x in lons_c for y in lats_c]\n",
    "    \n",
    "    geodf = gpd.GeoDataFrame(geometry=polygons)\n",
    "    \n",
    "    # for absolute consistency with grid cell centroids\n",
    "    geodf[\"lat_c\"] = [ y for x in lons_c for y in lats_c]\n",
    "    geodf[\"lon_c\"] = [ x for x in lons_c for y in lats_c]\n",
    "    \n",
    "    return geodf\n",
    "\n",
    "def get_me_ERA5_gridcell_weights(file, gridsize=0.25, full_data=False):\n",
    "    \n",
    "    '''\n",
    "    Function converts .geojson file of basin delineation\n",
    "    to weight matrix for further meteo forcing averaging\n",
    "    for using as input in lumped hydrological models\n",
    "    \n",
    "    Input:\n",
    "    1. file - .geojson polygon file.\n",
    "    You can import it using Qgis or \n",
    "    use geojson.io webservice.\n",
    "    2. gridsize (default=0.125) of ERA-Interim\n",
    "    global product resolution\n",
    "    3. full_data (default=False) return \n",
    "    geodataframe result (can be plotted)\n",
    "    Output:\n",
    "    1. Default (full_data=False): pandas dataframe\n",
    "    with ['lon', 'lat', 'weight'] columns\n",
    "    2. Extended (full_data=True): geopandas dataframe\n",
    "    with added \"geometry\" field\n",
    "    \n",
    "    source:\n",
    "    https://gis.stackexchange.com/questions/178765/intersecting-two-shapefiles-from-python-or-command-line\n",
    "\n",
    "    '''\n",
    "    # make geoDataFrame from basin file\n",
    "    watershed = gpd.read_file(file)\n",
    "    \n",
    "    # make geoDataFrame of grid cells matrix\n",
    "    with open(file) as f:\n",
    "        basin_json = geojson.loads(f.read())[\"features\"][0]\n",
    "    \n",
    "    # retrive geodataframe representation of grid cell polygons\n",
    "    grid = get_era5_polygons_geodf(get_boundaries(basin_json), gridsize=gridsize)\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for index, wtr in watershed.iterrows():\n",
    "        for index2, cell in grid.iterrows():\n",
    "            if wtr['geometry'].intersects(cell['geometry']):\n",
    "                data.append({'geometry': wtr['geometry'].intersection(cell['geometry']), \n",
    "                             'lon': cell['lon_c'],\n",
    "                             'lat': cell['lat_c'],\n",
    "                             'area': wtr['geometry'].intersection(cell['geometry']).area})\n",
    "\n",
    "    df = gpd.GeoDataFrame(data,columns=['geometry', 'lon', 'lat', 'area'])\n",
    "\n",
    "    # add weight for each gridcell\n",
    "    df['weight'] = df['area'] / df['area'].sum()\n",
    "    \n",
    "    # Because of global (!) ERA (and the same is valid for CMIP5) \n",
    "    # dataset has a longitute representation from 0 to 360\n",
    "    # instead of geographical projection representation from -180 to +180\n",
    "    # we need to add +180 to \"lon\" values \n",
    "    # for further valid data acqusition from global ERA datasets (tmin, tmax)\n",
    "    # link: https://software.ecmwf.int/wiki/display/CKB/ERA-Interim%3A+What+is+the+spatial+reference\n",
    "    # so, finally! ti is not so easy too!\n",
    "    # we need to perform backward convertion \n",
    "    # (according to forward convertion in get*polygons_geodf functions)\n",
    "    # previous (wrong) decision\n",
    "    # df['lon'] = df['lon'] + 180\n",
    "    #df[\"lon\"] = np.where(df[\"lon\"] < 0, df[\"lon\"] + 360, df[\"lon\"])\n",
    "        \n",
    "    # despite changes in df['lon'] values\n",
    "    # returned geometries with full_data=True are geographical (lon:[-180,180])!\n",
    "    if full_data:\n",
    "        return df\n",
    "    \n",
    "    return df[['lon', 'lat', 'weight']]\n",
    "\n",
    "def oudin2005(df_row):\n",
    "    \n",
    "    # climatic temperature was changed by observed one\n",
    "    \n",
    "    tmean, lat, doy = df_row[\"t2m\"], df_row[\"lat\"], df_row[\"doy\"]\n",
    "    \n",
    "    # Reference: http://www.fao.org/docrep/x0490e/x0490e07.htm\n",
    "    # use with caution for latitudes out of range 0-67 degrees\n",
    "\n",
    "    # Convert latitude [degrees] to radians\n",
    "    latrad = np.deg2rad(lat)\n",
    "\n",
    "    # Part 2. Extraterrrestrial radiation calculation\n",
    "    # set solar constant (in W m-2)\n",
    "    Rsc = 1367\n",
    "    # calculate solar declination dt (in radians)\n",
    "    dt = 0.409 * np.sin(2 * np.pi / 365 * doy - 1.39)\n",
    "    # calculate sunset hour angle (in radians)\n",
    "    # calculate sunset hour angle [radians]\n",
    "    # accorging to sunset_hour_angle_func() in PyET0\n",
    "    cos_ws = -np.tan(latrad) * np.tan(dt)\n",
    "    ws = np.arccos(min(max(cos_ws, -1.0), 1.0))\n",
    "    # Calculate sunshine duration N (in hours)\n",
    "    N = 24 / np.pi * ws\n",
    "    # Calculate day angle j (in radians)\n",
    "    j = 2 * np.pi / 365.25 * doy\n",
    "    # Calculate relative distance to sun\n",
    "    dr = 1.0 + 0.03344 * np.cos(j - 0.048869)\n",
    "    # Calculate extraterrestrial radiation (J m-2 day-1)\n",
    "    Re = Rsc * 86400 / np.pi * dr * (ws * np.sin(latrad) * np.sin(dt) + np.sin(ws) * np.cos(latrad) * np.cos(dt))\n",
    "    # convert from J m-2 day-1 to MJ m-2 day-1\n",
    "    Re = Re/10**6\n",
    "\n",
    "    # Part 3. Avearge daily temperatures calculation derived from long-term observations\n",
    "    #Ta = np.array([tmean[tmean.index.dayofyear == x].mean() for x in range(1, 367)])\n",
    "\n",
    "    # Part 4. PET main equation by (Oudin et al., 2005)\n",
    "    # lambda (latent heat flux const) = 2.45 MJ kg-1\n",
    "    # ro (density of water const) = 1000 kg m-3\n",
    "    # PE im m day -1 should be converted to mm/day (* 10**3)\n",
    "    # PE = ( Re / (2.45*1000) ) * ( (Ta+5) /100 ) * 10**3\n",
    "    # threshhold condition\n",
    "    # if Ta+5>0 - use Oudin formula, else set to zero\n",
    "    #PE = np.where(Ta+5 > 0, ( Re / (2.45*1000) ) * ( (Ta+5) /100 )*10**3, 0)\n",
    "    PE = np.where(tmean+5 > 0, ( Re / (2.45*1000) ) * ( (tmean+5) /100 )*10**3, 0)\n",
    "\n",
    "    return PE\n",
    "\n",
    "def delineation2list(data_instance, delineation_instance, variable):\n",
    "        \n",
    "    # handler for variable\n",
    "    # list of pandas.Series which than we will convert to dataframe\n",
    "    results_list = []\n",
    "    \n",
    "    #print(len(delineation_instance), \" nodes for extraction\")\n",
    "    for idx, row in delineation_instance.iterrows():\n",
    "        \n",
    "        #print(idx)\n",
    "        # obtain values from grid cells\n",
    "        point = data_instance[variable].sel(longitude=row[\"lon\"],\n",
    "                                            latitude=row[\"lat\"]).to_series() * row[\"weight\"]\n",
    "\n",
    "        # append obtained series to a list\n",
    "        results_list.append(point)\n",
    "    \n",
    "    # close data reference\n",
    "    # data_instance.close()\n",
    "    \n",
    "    return results_list\n",
    "\n",
    "def temp2pet(tmean_list, delineation_instance):\n",
    "    \n",
    "    # handler for variable\n",
    "    # list of pandas.Series which than we will convert to dataframe\n",
    "    pet_list = []\n",
    "    \n",
    "    for idx, row in delineation_instance.iterrows():\n",
    "        \n",
    "        tmean = tmean_list[idx] / row[\"weight\"]\n",
    "               \n",
    "        df = pd.DataFrame(tmean)\n",
    "        \n",
    "        #print(tmean)\n",
    "        \n",
    "        df[\"lat\"] = row[\"lat\"]\n",
    "        \n",
    "        #df.rename(index=str, columns={\"t2m\":\"tmean\", \"lat\": \"lat\"}, inplace=True)\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df[\"doy\"] = df.index.dayofyear     \n",
    "        \n",
    "        df[\"pet\"] = df.apply(oudin2005, axis=1)\n",
    "        \n",
    "        # append obtained series to a list\n",
    "        pet_list.append(df[\"pet\"] * row[\"weight\"])\n",
    "       \n",
    "    return pet_list\n",
    "\n",
    "def extract_ERA_reanalisys_data(geojson_file):\n",
    "    \n",
    "    # calculate delineation   \n",
    "    ERA_delineation = get_me_ERA5_gridcell_weights(file=geojson_file)\n",
    "    \n",
    "    # read the data\n",
    "    ERA_precipitation = xr.open_dataset(\"../data/era5_uk_daily_prec.nc\")\n",
    "    ERA_temp_mean = xr.open_dataset(\"../data/era5_uk_daily_temp.nc\")\n",
    "    \n",
    "    # ERA precipitation data\n",
    "    precipitation_list = delineation2list(data_instance=ERA_precipitation, \n",
    "                                          delineation_instance=ERA_delineation, variable=\"tp\")\n",
    "    \n",
    "    # ERA minimum temperature data\n",
    "    temperature_mean_list = delineation2list(data_instance=ERA_temp_mean,\n",
    "                                             delineation_instance=ERA_delineation, variable=\"t2m\")    \n",
    "    \n",
    "    #return precipitation_list, temperature_min_list, temperature_max_list\n",
    "    evaporation_list = temp2pet(tmean_list=temperature_mean_list,\n",
    "                                delineation_instance=ERA_delineation)\n",
    "    \n",
    "    # create dataframes and store our averaged series\n",
    "    lumped_out = pd.DataFrame( {'Temp': pd.concat(temperature_mean_list, axis=1).sum(axis=1),\n",
    "                                'Prec': pd.concat(precipitation_list, axis=1).sum(axis=1), \n",
    "                                'Evap': pd.concat(evaporation_list, axis=1).sum(axis=1)} )\n",
    "    \n",
    "    #lumped_out[\"Temp\"] = lumped_out[\"Tmin\"] + lumped_out[\"Tmax\"] / 2.0\n",
    "        \n",
    "    #return temperature_mean_list\n",
    "    return lumped_out\n",
    "\n",
    "def hm_calibration(data_instance, model, bounds, seed=1010):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calibration meta function for hydrologic models\n",
    "    \n",
    "    Input :\n",
    "    1. data_instance: pandas dataframe with data:\n",
    "        \"Obs\"   : observed streamflow (mm/day)\n",
    "        \"Temp\"   : mean daily temperature (Celsius)\n",
    "        \"Prec\"   : daily sum of precipitation (mm)\n",
    "        \"Evap\" : daily potential evapotranspiration (mm)\n",
    "    2. model :\n",
    "        function for running hydrologic model\n",
    "    3. bounds :\n",
    "        function for deriving lower and upper bounds\n",
    "        of model parameters for calibration\n",
    "    4. seed :\n",
    "        random seed number for differential evolution algorithm\n",
    "    \n",
    "    Output:\n",
    "    #1. runoff :\n",
    "        Simulated (Qsim) runoff\n",
    "    2. opt_par :\n",
    "        optimal set of model parameters\n",
    "    #3. NS :\n",
    "        Nash-Sutcliffe efficiency criteria\n",
    "    \"\"\"\n",
    "      \n",
    "    _Q   = data_instance[\"Obs\"].values\n",
    "    _T   = data_instance[\"Temp\"].values\n",
    "    _P   = data_instance[\"Prec\"].values\n",
    "    _PET = data_instance[\"Evap\"].values\n",
    "    \n",
    "    # add warmup\n",
    "    # simply full period\n",
    "    \n",
    "    Qobs = np.concatenate([_Q,  _Q])\n",
    "    T    = np.concatenate([_T,  _T])\n",
    "    P    = np.concatenate([_P,  _P])\n",
    "    PET  = np.concatenate([_PET, _PET])\n",
    "    \n",
    "    def NS_loss(params):\n",
    "        # simulate hydrograph\n",
    "        Qsim = model(T, P, PET, params)\n",
    "        Qm = np.nanmean(Qobs)\n",
    "        # calculate objective function value\n",
    "        return np.nansum((Qobs-Qsim)**2)/np.nansum((Qobs-Qm)**2)\n",
    "    \n",
    "    result = differential_evolution(NS_loss, \n",
    "                                    bounds=bounds(), \n",
    "                                    maxiter=1000, \n",
    "                                    polish=True, \n",
    "                                    disp=False, \n",
    "                                    seed=seed)\n",
    "    \n",
    "    opt_par = result.x\n",
    "    \n",
    "    Qsim = model(T, P, PET, opt_par)\n",
    "    \n",
    "    # cut the warmup period\n",
    "    Qsim = Qsim[len(_Q):].copy()\n",
    "    Qobs = Qobs[len(_Q):].copy()\n",
    "    \n",
    "    NS = 1 - np.nansum((Qobs-Qsim)**2)/np.nansum((Qobs-np.nanmean(Qobs))**2)\n",
    "    \n",
    "    #print(\"NS : {}\".format(np.round(NS, 2)))\n",
    "    \n",
    "    #runoff = pd.DataFrame({\"Qobs\": Qobs, \"Qsim\": Qsim}, index=data_instance.index)\n",
    "    \n",
    "    return Qsim, opt_par, NS\n",
    "\n",
    "def generate_future_data(data_instance):\n",
    "    \n",
    "    observed_rainfall = data_instance[\"Prec\"]\n",
    "    \n",
    "    Tclim   = np.array([data_instance[\"Temp\"].loc[data_instance[\"Temp\"].index.dayofyear == doy].mean() for doy in range(1, 367)])\n",
    "    PETclim = np.array([data_instance[\"Evap\"].loc[data_instance[\"Evap\"].index.dayofyear == doy].mean() for doy in range(1, 367)])\n",
    "    \n",
    "    future_dates = pd.date_range(\"2016-01-01\", \"2020-12-2\", freq=\"D\")\n",
    "    \n",
    "    lenght = len(future_dates)\n",
    "    \n",
    "    df = pd.DataFrame(index=future_dates)\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    # 1. Constant mean obs rain\n",
    "    df[\"MeanRain\"] = np.ones(lenght) * np.median(observed_rainfall)\n",
    "    \n",
    "    # 2. Constant max obs rain\n",
    "    df[\"MaxRain\"] = np.ones(lenght) * np.max(observed_rainfall)\n",
    "    \n",
    "    # 3. Random sampling from distribution\n",
    "    df[\"RandomRain\"] = np.random.random(lenght) * np.max(observed_rainfall)\n",
    "    \n",
    "    df[\"Temp\"] = np.array([Tclim[i.dayofyear-1] for i in df.index])\n",
    "    \n",
    "    df[\"Evap\"] = np.array([PETclim[i.dayofyear-1] for i in df.index])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def df2html(df, df_future, station_number, station_name):\n",
    "    \n",
    "    title = f\"{station_name}\"\n",
    "    \n",
    "    p1 = figure(title=title, x_axis_type=\"datetime\", plot_width=500, plot_height=300)\n",
    "    p1.grid.grid_line_alpha=0.5\n",
    "    p1.xaxis.axis_label = 'Date'\n",
    "    p1.yaxis.axis_label = 'Runoff, mm/day'\n",
    "\n",
    "    p1.line(df.index, df[\"Sim\"], color='deepskyblue', legend='Historical')\n",
    "    p1.line(df_future.index, df_future[\"MeanRunoff\"], color='seagreen', legend='MeanRainRunoff')\n",
    "    p1.line(df_future.index, df_future[\"MaxRunoff\"], color='crimson', legend='MaxRainRunoff')\n",
    "    p1.line(df_future.index, df_future[\"RandomRunoff\"], color='turquoise', legend='RandomRainRunoff')\n",
    "    p1.legend.location = \"top_left\"\n",
    "\n",
    "    output_file(f\"../docs/{station_number}.html\", title=\"4Y11M2D\")\n",
    "    save_html(p1)\n",
    "    \n",
    "    #show(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_html(station_id, station_name):\n",
    "    \n",
    "    path_runoff = f\"../data/runoff/{station_id}.pkl\"\n",
    "    path_shape  = f\"../data/shapes/{station_id}.geojson\"\n",
    "    \n",
    "    runoff = pd.read_pickle(path_runoff)\n",
    "    shape  = gpd.read_file(path_shape)\n",
    "    \n",
    "    meteo = extract_ERA_reanalisys_data(path_shape)\n",
    "    \n",
    "    data = pd.concat([runoff, meteo], axis=1)\n",
    "    \n",
    "    qsim, opt_par, ns = hm_calibration(data, gr4j, gr4j_bounds, seed=42)\n",
    "    \n",
    "    data[\"Sim\"] = qsim\n",
    "    \n",
    "    data_future = generate_future_data(data)\n",
    "    \n",
    "    for rain in [\"MeanRain\", \"MaxRain\", \"RandomRain\"]:\n",
    "    \n",
    "        data_future[rain[:-4]+\"Runoff\"] = gr4j(np.concatenate([data[\"Temp\"], data_future[\"Temp\"]]), \n",
    "                                               np.concatenate([data[\"Prec\"], data_future[rain]]), \n",
    "                                               np.concatenate([data[\"Evap\"], data_future[\"Evap\"]]), opt_par)[len(data[\"Prec\"]):]\n",
    "    \n",
    "    df2html(data, data_future, station_id, station_name)\n",
    "    \n",
    "    return ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.67\n",
      "1: 0.85\n",
      "2: 0.69\n",
      "3: 0.48\n",
      "4: 0.48\n",
      "5: 0.44\n",
      "6: 0.45\n",
      "7: 0.4\n",
      "8: 0.47\n",
      "9: 0.68\n",
      "10: 0.59\n",
      "11: 0.52\n",
      "12: 0.44\n",
      "13: 0.39\n",
      "14: 0.46\n",
      "15: 0.49\n",
      "16: 0.46\n",
      "17: 0.4\n",
      "18: 0.52\n",
      "19: 0.58\n",
      "20: 0.53\n",
      "21: 0.47\n",
      "22: 0.58\n",
      "23: 0.59\n",
      "24: 0.59\n",
      "25: 0.59\n",
      "26: 0.5\n",
      "27: 0.56\n",
      "28: 0.7\n",
      "29: 0.71\n",
      "30: 0.73\n",
      "31: 0.84\n",
      "32: 0.81\n",
      "33: 0.65\n",
      "34: 0.71\n",
      "35: 0.68\n",
      "36: 0.81\n",
      "37: 0.85\n",
      "38: 0.54\n",
      "39: 0.51\n",
      "40: 0.77\n",
      "41: 0.75\n",
      "42: 0.77\n",
      "43: 0.72\n",
      "44: 0.65\n",
      "45: 0.59\n",
      "46: 0.58\n",
      "47: 0.52\n",
      "48: 0.56\n",
      "49: 0.69\n",
      "50: 0.55\n",
      "51: 0.65\n",
      "52: 0.61\n",
      "53: 0.72\n",
      "54: 0.67\n",
      "55: 0.69\n",
      "56: 0.71\n",
      "57: 0.63\n",
      "58: 0.6\n",
      "59: 0.68\n",
      "60: 0.71\n",
      "61: 0.76\n",
      "62: 0.72\n",
      "63: 0.72\n",
      "64: 0.74\n",
      "65: 0.74\n",
      "66: 0.8\n",
      "67: 0.78\n",
      "68: 0.55\n",
      "69: 0.76\n",
      "70: 0.79\n",
      "71: 0.78\n",
      "72: 0.66\n",
      "73: 0.74\n",
      "74: 0.57\n",
      "75: 0.69\n",
      "76: 0.67\n",
      "77: 0.74\n",
      "78: 0.52\n",
      "79: 0.77\n",
      "80: 0.71\n",
      "81: 0.71\n",
      "82: 0.83\n",
      "83: 0.73\n",
      "84: 0.71\n",
      "85: 0.47\n",
      "86: 0.8\n",
      "87: 0.65\n",
      "88: 0.63\n",
      "89: 0.78\n",
      "90: 0.81\n",
      "91: 0.76\n",
      "92: 0.79\n",
      "93: 0.81\n",
      "94: 0.76\n",
      "95: 0.67\n",
      "96: 0.76\n",
      "97: 0.67\n",
      "98: 0.76\n",
      "99: 0.78\n",
      "100: 0.72\n",
      "101: 0.75\n",
      "102: 0.7\n",
      "103: 0.61\n",
      "104: 0.72\n",
      "105: 0.7\n",
      "106: 0.79\n",
      "107: 0.82\n",
      "108: 0.73\n",
      "109: 0.77\n",
      "110: 0.83\n",
      "111: 0.82\n",
      "112: 0.74\n",
      "113: 0.83\n",
      "114: 0.69\n",
      "115: 0.75\n",
      "116: 0.7\n",
      "117: 0.61\n",
      "118: 0.69\n",
      "119: 0.7\n",
      "120: 0.64\n",
      "121: 0.72\n",
      "122: 0.8\n",
      "123: 0.79\n",
      "124: 0.76\n",
      "125: 0.72\n",
      "126: 0.73\n",
      "127: 0.79\n",
      "128: 0.88\n",
      "129: 0.71\n",
      "130: 0.86\n",
      "131: 0.64\n",
      "132: 0.84\n",
      "133: 0.9\n",
      "134: 0.82\n",
      "135: 0.77\n",
      "136: 0.82\n",
      "137: 0.85\n",
      "138: 0.82\n",
      "139: 0.71\n",
      "140: 0.68\n",
      "141: 0.84\n",
      "142: 0.72\n",
      "143: 0.74\n",
      "144: 0.75\n",
      "145: 0.74\n",
      "146: 0.75\n",
      "147: 0.71\n",
      "148: 0.58\n",
      "149: 0.63\n",
      "150: 0.73\n",
      "151: 0.7\n",
      "152: 0.77\n",
      "153: 0.87\n",
      "154: 0.83\n",
      "155: 0.73\n",
      "156: 0.85\n",
      "157: 0.86\n",
      "158: 0.9\n",
      "159: 0.85\n",
      "160: 0.84\n",
      "161: 0.77\n",
      "162: 0.83\n",
      "163: 0.76\n",
      "164: 0.9\n",
      "165: 0.65\n",
      "166: 0.88\n",
      "167: 0.89\n",
      "168: 0.84\n",
      "169: 0.63\n",
      "170: 0.75\n",
      "171: 0.71\n",
      "172: 0.69\n",
      "173: 0.64\n",
      "174: 0.5\n",
      "175: 0.8\n",
      "176: 0.81\n",
      "177: 0.83\n",
      "178: 0.69\n",
      "179: 0.84\n",
      "180: 0.74\n",
      "181: 0.8\n",
      "182: 0.8\n",
      "183: 0.78\n",
      "184: 0.73\n",
      "185: 0.73\n",
      "186: 0.67\n",
      "187: 0.76\n",
      "188: 0.8\n",
      "189: 0.83\n",
      "190: 0.81\n",
      "191: 0.73\n",
      "192: 0.67\n",
      "193: 0.77\n",
      "194: 0.83\n",
      "195: 0.74\n",
      "196: 0.83\n",
      "197: 0.83\n",
      "198: 0.79\n",
      "199: 0.76\n",
      "200: 0.78\n",
      "201: 0.85\n",
      "202: 0.82\n",
      "203: 0.76\n",
      "204: 0.8\n",
      "205: 0.82\n",
      "206: 0.73\n",
      "207: 0.76\n",
      "208: 0.72\n",
      "209: 0.76\n",
      "210: 0.89\n",
      "211: 0.8\n",
      "212: 0.83\n",
      "213: 0.62\n",
      "214: 0.67\n",
      "215: 0.79\n",
      "216: 0.63\n",
      "217: 0.76\n",
      "218: 0.81\n",
      "219: 0.79\n",
      "220: 0.79\n",
      "221: 0.8\n",
      "222: 0.84\n",
      "223: 0.64\n",
      "224: 0.75\n",
      "225: 0.81\n",
      "226: 0.78\n",
      "227: 0.81\n",
      "228: 0.79\n",
      "229: 0.73\n",
      "230: 0.74\n",
      "231: 0.76\n",
      "232: 0.59\n",
      "233: 0.66\n",
      "234: 0.55\n",
      "235: 0.67\n",
      "236: 0.71\n",
      "237: 0.72\n",
      "238: 0.74\n",
      "239: 0.78\n",
      "240: 0.72\n",
      "241: 0.81\n",
      "242: 0.74\n",
      "243: 0.84\n",
      "244: 0.6\n",
      "245: 0.55\n",
      "246: 0.67\n",
      "247: 0.66\n",
      "248: 0.82\n",
      "249: 0.66\n",
      "250: 0.57\n",
      "251: 0.78\n",
      "252: 0.53\n",
      "253: 0.66\n",
      "254: 0.75\n",
      "255: 0.78\n",
      "256: 0.77\n",
      "257: 0.8\n",
      "258: 0.8\n",
      "259: 0.75\n",
      "260: 0.72\n",
      "261: 0.59\n",
      "262: 0.59\n",
      "263: 0.62\n",
      "264: 0.56\n",
      "265: 0.83\n",
      "266: 0.66\n",
      "267: 0.6\n",
      "268: 0.64\n",
      "269: 0.58\n",
      "270: 0.63\n",
      "271: 0.7\n",
      "272: 0.6\n",
      "273: 0.69\n",
      "274: 0.7\n",
      "275: 0.82\n",
      "276: 0.6\n",
      "277: 0.65\n",
      "278: 0.63\n",
      "279: 0.81\n",
      "280: 0.76\n",
      "281: 0.79\n",
      "282: 0.81\n",
      "283: 0.73\n",
      "284: 0.78\n",
      "285: 0.81\n",
      "286: 0.73\n",
      "287: 0.71\n",
      "288: 0.71\n",
      "289: 0.72\n",
      "290: 0.66\n",
      "291: 0.82\n",
      "292: 0.83\n",
      "293: 0.75\n",
      "294: 0.67\n",
      "295: 0.7\n",
      "296: 0.66\n",
      "297: 0.66\n",
      "298: 0.71\n",
      "299: 0.72\n",
      "300: 0.89\n",
      "301: 0.76\n",
      "302: 0.5\n",
      "303: 0.66\n",
      "304: 0.74\n",
      "305: 0.78\n",
      "306: 0.69\n",
      "307: 0.58\n",
      "308: 0.63\n",
      "309: 0.71\n",
      "310: 0.74\n",
      "311: 0.7\n",
      "312: 0.73\n",
      "313: 0.78\n",
      "314: 0.76\n",
      "315: 0.88\n",
      "CPU times: user 24min 49s, sys: 10.5 s, total: 24min 59s\n",
      "Wall time: 25min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "efficiencies = []\n",
    "for idd, row in meta_data.iterrows():\n",
    "        \n",
    "    idx = row[\"STATION\"]\n",
    "    name = row[\"NAME\"]\n",
    "    \n",
    "    nss = generate_html(idx, name)\n",
    "    \n",
    "    efficiencies.append( nss )\n",
    "    \n",
    "    print(f\"{idd}: {np.round(nss, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'NS')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "efficiencies = np.array(efficiencies)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.boxplot(efficiencies)\n",
    "\n",
    "#https://stackoverflow.com/questions/12998430/remove-xticks-in-a-matplotlib-plot\n",
    "plt.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False) # labels along the bottom edge are off\n",
    "\n",
    "plt.xlabel(\"GR4J\")\n",
    "plt.ylabel(\"NS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72771207721941167"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(efficiencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
